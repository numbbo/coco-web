---
title: Getting started---Benchmarking an algorithm (collecting data) with Python
---
We use the [`cocoex`](https://pypi.org/project/cocoex) Python module to benchmark an algorithm.
For bencharking in a different language than Python, see [here](https://github.com/numbbo/coco).

### Installation (assuming Python is present)
From a system shell, execute

```sh
python -m pip install cocoex cocopp
```

### Using [`cocoex`](https://numbbo.github.io/gforge/apidocs-cocoex)
Depending on the algorithm, we have to chose the appropriate benchmark suite

```python
>>> import cocoex

>>> cocoex.known_suite_names
['bbob',
 'bbob-biobj',
 'bbob-biobj-ext',
 'bbob-constrained',
 'bbob-largescale',
 'bbob-mixint',
 'bbob-biobj-mixint']
```

see also [here](https://numbbo.github.io/coco/testsuites).

A [short-ish example code](https://github.com/numbbo/coco/blob/development/code-experiments/build/python/example/example_experiment_for_beginners.py) that benchmarks [`scipy.optimize.fmin`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html) on the [`bbob`](https://numbbo.github.io/coco/testsuites/bbob) suite looks like this:


```python
import cocoex, cocopp  # experimentation and post-processing modules
import scipy.optimize  # to define the solver to be benchmarked

### input
suite_name = "bbob"
output_folder = "scipy-optimize-fmin"
fmin = scipy.optimize.fmin  # optimizer to be benchmarked
budget_multiplier = 1.1  # increase to 3, 10, 30, ... x (dimension+1)

### prepare
suite = cocoex.Suite(suite_name, "", "")  # see https://numbbo.github.io/coco-doc/C/#suite-parameters
observer = cocoex.Observer(suite_name, "result_folder: " + output_folder)
# cocoex.TrackEvalsAndSuccesses = cocoex.TrackEvalsAndSuccessesDummy
tracker = cocoex.TrackEvalsAndSuccesses(budget_multiplier)
minimal_print = cocoex.utilities.MiniPrint()

### go
while not tracker.done():  # while budget is left and successes are missing
    for problem in suite:  # this loop will take 2-3 minutes x budget_multiplier
        if tracker.done(problem):
            continue
        problem.observe_with(observer)  # generates the data for cocopp
        problem(problem.dimension * [0])  # improve comparability to existing data
        xopt = fmin(problem, tracker.initial_solution_proposal(problem), disp=False)
        problem(xopt)  # make sure the returned solution is evaluated
        tracker.track(problem)
        minimal_print(problem, final=problem.index == len(suite) - 1)

### post-process data
cocopp.main(observer.result_folder + ' nelder!');  # re-run folders look like "...-001" etc
```

The benchmarking data are written to a subfolder in the `exdata` folder.
The last line postprocesses the obtained data and compares the result with BFGS.
The resulting figures and tables can be browsed via `ppdata/index.html`.

For benchmarking another algorithm,

1. copy-paste the above code in a file, say `benchmark_experiment.py`,
2. revise the lines following `### input` and in particular reassign `fmin` to run the desired algorithm,
3. adapt the calling code `fmin(problem, ...)` respectively.
   For example, some algorithms may need a maximal budget, like `(problem.dimension + 1) * budget_multiplier` as input parameter,
4. execute the file from a system shell like 
   ```python
   python benchmark_experiment.py
   ```
   or from an ipython-shell or notebook like 
   ```python
   %run benchmark_experiment.py
   ```

CAVEAT: the `cocoex.TrackEvalsAndSuccesses` class is yet to be released.

